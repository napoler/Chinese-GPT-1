# -*- coding: utf-8 -*-
"""Chinese_GPT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cvBSt2uF7hYL1feDGt0dkCxIeaVXQs5x

## Install the required package
"""

# for google drive
# !pip install -U -q PyDrive

# !pip install cupy-cuda100

# !pip install chinese-gpt

# """## Download the pretrained weights"""

# from pydrive.auth import GoogleAuth
# from pydrive.drive import GoogleDrive
# from google.colab import auth
# from oauth2client.client import GoogleCredentials

# # 1. Authenticate and create the PyDrive client.
# auth.authenticate_user()
# gauth = GoogleAuth()
# gauth.credentials = GoogleCredentials.get_application_default()
# drive = GoogleDrive(gauth)

# file_id = '1W6n7Kv6kvHthUX18DhdGSzBYkyzDvxYh'
# downloaded = drive.CreateFile({'id': file_id})
# downloaded.GetContentFile('model_state_epoch_62.th')

"""## Run the model"""

import numpy as np
import pandas as pd

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader

import time
from tqdm import tqdm
import itertools

# uses allennlp modules
from allennlp.nn import util
from allennlp.nn.beam_search import BeamSearch

# imports chinese gpt
from chinese_gpt import TransformerDecoderLM

# uses bert chinese wordpiece tokenization
from pytorch_pretrained_bert import BertTokenizer

def top_k_logits(logits, k):
    """Mask logits so that only top-k logits remain
    """
    values, _ = torch.topk(logits, k)
    min_values = values[:, -1].unsqueeze(1).repeat(1, logits.shape[-1])
    return torch.where(logits < min_values, torch.ones_like(logits, dtype=logits.dtype) * -1e10, logits)


def load_model():
    """### Define Bert tokenizer"""

    tokenizer = BertTokenizer.from_pretrained("bert-base-chinese")

    # sentence = "今天北京天气出现小雨,山区还出现了降雪,气温下降,体感十分寒冷。"
    # print("Tokenize")
    # print(tokenizer.tokenize(sentence))
    # print("Tokens to ids")
    # ids = tokenizer.convert_tokens_to_ids(sentence)
    # print(ids)
    # print("Ids to tokens")
    # print(tokenizer.convert_ids_to_tokens(ids))

    # source = '[CLS]' + sentence + '[SEP]'
    # target = '[CLS]' + sentence + '[SEP]'


    # make sure your model is on GPU
    device = torch.device("cpu")

    model = TransformerDecoderLM()
    model = model.to(device)

    """### Load weights into the model"""

    old_state_dict = torch.load("model/model_state_epoch_62.th", map_location=lambda storage, loc: storage)
    new_state_dict = model.state_dict()

    for item in new_state_dict.keys():
        new_state_dict[item] = old_state_dict['module.'+item]
        
    model.load_state_dict(new_state_dict)

    return model,tokenizer



# model,tokenizer =load_model()

def pre(text,len,model,tokenizer):
    """### Conditioanl or Unconditional Decoding"""
    batch_size = 1

    # make sure your model is on GPU
    device = torch.device("cpu")   
    # ask more about news
    prompt = tokenizer.tokenize(text)
    prompt = tokenizer.convert_tokens_to_ids(prompt)

    top_k = 50
    temperature = 1.0
    length = 0

    start_predictions = torch.LongTensor([[101] + prompt]* batch_size).to(device)
    mask = torch.ones(batch_size, start_predictions.shape[1]).to(device)

    with torch.no_grad():
        # cache saves in past
        logits, past = model(start_predictions, mask, past=None, past_length=0)
        logits = logits[:, -1, :] / temperature
        logits = top_k_logits(logits, k=top_k)

        sentence = []

        probs = F.softmax(logits, dim=-1)
        prob, prev_pred = torch.topk(probs, k=1, dim=-1)
        sentence.append(prev_pred)
        length += 1

        # decoding loop
        for i in tqdm(range(len)):
            mask = F.pad(mask, (0, 1), "constant", 1.0)
            logits, past = model(prev_pred, mask, past=past, past_length=length)
            logits = logits.squeeze(1) / temperature
            logits = top_k_logits(logits, k=top_k)
            probs = F.softmax(logits, dim=-1)
            prev_pred = torch.multinomial(probs, num_samples=1)
            sentence.append(prev_pred)
            length += 1

        sentence = torch.cat(sentence, dim=-1)

        res = "".join(tokenizer.convert_ids_to_tokens(sentence[0].tolist()))
        output=[]
        for i in range(0, 512, 128):
            output.append(res[i:i+128])
        return "".join(output)

model,tokenizer =load_model()
text="柯基犬可是个"
p_text=pre(text=text,len=500,model=model,tokenizer=tokenizer)
print(text)
print(p_text)
# !nvidia-smi

